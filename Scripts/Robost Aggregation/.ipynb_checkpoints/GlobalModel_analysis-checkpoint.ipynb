{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edde1d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bff0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('Globalmodel_FederatedAveraging.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae309c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lp1 = pd.read_excel('Laptop1_preprocessedDS.xlsx',engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5757d83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lp2 = pd.read_excel('Laptop2_preprocessedDS.xlsx', engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e615909",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lp3 = pd.read_excel('Laptop3_preprocessedDS.xlsx', engine = 'openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb4606d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_lp1, df_lp2, df_lp3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325e4ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_excel(\"AllLaptopsDS.xlsx\", engine='openpyxl')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524b6b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    \n",
    "    # Evaluating on X_test checks model accuracy on new data;\n",
    "    # 0.5 is a threshold for binary outcomes\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test).ravel() \n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)    \n",
    "    \n",
    "    print(\"range(len(y_test)\")\n",
    "    print(range(len(y_test)))\n",
    "    print(\"range(len(y_pred)\")\n",
    "    print(range(len(y_pred)))\n",
    "          \n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plotting the observed (actual) values vs predicted probabilities\n",
    "    observed = plt.scatter(range(len(y_test)), y_test, color='red', label='Observed', alpha=0.5, marker='o')\n",
    "\n",
    "    # Plotting the same for predicted binary outcomes for comparison\n",
    "    predicted = plt.scatter(range(len(y_pred_binary)), y_pred_binary, color='blue', label='Predicted', alpha=0.5, marker='x')\n",
    "\n",
    "    # Draw a line of best fit for observed vs predicted\n",
    "    m, b = np.polyfit(range(len(y_test)), y_pred, 1)\n",
    "    plt.plot(range(len(y_test)), m*np.arange(len(y_test)) + b, color='green', label='Fit')\n",
    "\n",
    "    # Enhance plot\n",
    "    plt.title('Observed vs Predicted')\n",
    "    plt.xlabel('Samples')\n",
    "    plt.ylabel('Output label')\n",
    "    plt.legend(handles=[observed, predicted])\n",
    "    plt.show()  \n",
    "    \n",
    "    # confusion matrix\n",
    "    # https://neptune.ai/blog/evaluation-metrics-binary-classification\n",
    "    print(\"confusion matrix\")\n",
    "    cm = confusion_matrix(y_test, y_pred_binary)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    false_positive_rate = fp / (fp + tn)\n",
    "    print(f\"False Positive Rate: {false_positive_rate}\")\n",
    "    false_negative_rate = fn / (tp + fn)\n",
    "    print(f\"False Negative Rate: {false_negative_rate}\")\n",
    "    true_negative_rate = tn / (tn + fp)\n",
    "    print(f\"True Negative Rate: {true_negative_rate}\")   \n",
    "\n",
    "    if (tn + fn) > 0:\n",
    "        negative_predictive_value = tn / (tn + fn)\n",
    "        print(f\"Negative Prediction Value: {negative_predictive_value}\")\n",
    "    else:\n",
    "        print(\"Negative Prediction Value: Undefined (no true or false negatives)\")\n",
    "    \n",
    "    if (tp + fp) == 0:\n",
    "        false_discovery_rate = 0\n",
    "        print(f\"False Discovery Rate: {false_discovery_rate}\")  \n",
    "    else:\n",
    "        false_discovery_rate = fp/ (tp + fp)\n",
    "        print(f\"False Discovery Rate: {false_discovery_rate}\") \n",
    "        \n",
    "    recall = recall_score(y_test, y_pred_binary) \n",
    "    print(f\"recall: {recall}\")\n",
    "    precision = precision_score(y_test, y_pred_binary) \n",
    "    print(f\"precision: {precision}\")    \n",
    "    fbeta = fbeta_score(y_test, y_pred_binary, beta=2)\n",
    "    print(f\"fbeta: {fbeta}\")\n",
    "    f1= f1_score(y_test, y_pred_binary)\n",
    "    print(f\"f1: {f1}\")\n",
    "    f2 = fbeta_score(y_test, y_pred_binary, beta = 2)\n",
    "    print(f\"f2: {f2}\")\n",
    "    cohen_kappa = cohen_kappa_score(y_test, y_pred_binary)\n",
    "    print(f\"cohen_kappa: {cohen_kappa}\") \n",
    "    \n",
    "    \n",
    "    # Classification report (precision, recall, F1-score)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred_binary, zero_division=0))\n",
    "\n",
    "    # ROC-AUC score\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(y_test, y_pred)\n",
    "        print(f\"ROC_AUC score: {roc_auc}\")\n",
    "        \n",
    "    except:\n",
    "        # Handle cases where ROC-AUC can not be computed\n",
    "        print(\"ROC-AUC score cannot be computed for this model configuration.\")\n",
    "        \n",
    "    # Matthews Correlation Coefficient (MCC)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred_binary)\n",
    "    print(f\"Matthews Correlation Coefficient (MCC): {mcc}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e3d6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"Data preprocessing complete.\")\n",
    "\n",
    "print(\"Compiling and training the model.\")\n",
    "# for binary classification task\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=12)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"Model accuracy: {accuracy}\") \n",
    "\n",
    "evaluate_model(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9176294",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d945f536",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031b903b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea438083",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999181b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
