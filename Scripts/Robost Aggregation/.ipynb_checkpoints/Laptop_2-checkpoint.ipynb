{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa096c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client program started.\n",
      "Connected to server at 127.0.0.1:5300\n",
      "Starting to receive the model structure.\n",
      "End of model structure reached.\n",
      "Model structure received. Sending acknowledgment.\n",
      "Expected length of initial model weights data: 12352 bytes.\n",
      "Initial model weights received.\n",
      "Model loaded successfully with received structure and weights.\n",
      "Initial size: (801310, 5)\n",
      "Size after dropping NA: (801309, 5)\n",
      "Data preprocessing complete.\n",
      "Compiling and training the model.\n",
      "Epoch 1/10\n",
      "\u001b[1m46757/46757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1ms/step - accuracy: 0.9026 - loss: 0.1890\n",
      "Epoch 2/10\n",
      "\u001b[1m46757/46757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 1ms/step - accuracy: 0.9984 - loss: 0.0081\n",
      "Epoch 3/10\n",
      "\u001b[1m46757/46757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 1ms/step - accuracy: 0.9994 - loss: 0.0037\n",
      "Epoch 4/10\n",
      "\u001b[1m46757/46757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 1ms/step - accuracy: 0.9997 - loss: 0.0021\n",
      "Epoch 5/10\n",
      "\u001b[1m46757/46757\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 1ms/step - accuracy: 0.9997 - loss: 0.0014\n",
      "Epoch 6/10\n",
      "\u001b[1m22601/46757\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 1ms/step - accuracy: 0.9998 - loss: 8.9173e-04"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import pickle\n",
    "from sklearn.metrics import classification_report, roc_auc_score, matthews_corrcoef\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score, f1_score, fbeta_score, cohen_kappa_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def receive_model(client_socket):   \n",
    "    print(\"Starting to receive the model structure.\")\n",
    "    model_json_parts = []\n",
    "    \n",
    "    # Initialize a variable to accumulate the model JSON parts\n",
    "    while True:\n",
    "        part = client_socket.recv(1024).decode()\n",
    "        model_json_parts.append(part)        \n",
    "        if len(part) < 1024:\n",
    "            print(\"End of model structure reached.\")\n",
    "            break  # Exit loop if last part of model structure is received\n",
    "    \n",
    "    # Combine the parts to form the full model JSON\n",
    "    model_json = ''.join(model_json_parts)\n",
    "    print(\"Model structure received. Sending acknowledgment.\")\n",
    "    client_socket.send(\"ACK_MODEL\".encode())  # Send acknowledgment for model structure\n",
    "    \n",
    "    # Receive the length of the initial model weights data\n",
    "    length_bytes = client_socket.recv(8)  # Length is expected to be sent as 8 bytes\n",
    "    length = int.from_bytes(length_bytes, byteorder='big')\n",
    "    print(f\"Expected length of initial model weights data: {length} bytes.\")\n",
    "    \n",
    "    # Initialize an empty byte string to accumulate the weights data\n",
    "    initial_weights_data = b\"\"\n",
    "    while len(initial_weights_data) < length:\n",
    "        # Calculate the remaining bytes to read\n",
    "        remaining_bytes = length - len(initial_weights_data)\n",
    "        packet = client_socket.recv(min(4096, remaining_bytes))\n",
    "        if not packet:\n",
    "            raise Exception(\"Connection closed prematurely while receiving initial model weights.\")\n",
    "        initial_weights_data += packet\n",
    "    \n",
    "    # Unpickle the received weights data\n",
    "    initial_weights = pickle.loads(initial_weights_data)\n",
    "    print(\"Initial model weights received.\")\n",
    "    \n",
    "    # Load the model structure from JSON and set the received weights\n",
    "    model = tf.keras.models.model_from_json(model_json)\n",
    "    model.set_weights(initial_weights)\n",
    "    print(\"Model loaded successfully with received structure and weights.\")\n",
    "    \n",
    "    return model\n",
    "    \n",
    "def preprocess_data_Train_Model(model):    \n",
    "    # Load your dataset\n",
    "    df = pd.read_csv('Traffic_Crashes_Crashes_Chicago_DS.csv')\n",
    "    # Optionally, check the initial size of the DataFrame\n",
    "    print(\"Initial size:\", df.shape)\n",
    "\n",
    "    # Drop rows with missing values more selectively if needed\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Check the size after dropping NA to ensure you still have enough data\n",
    "    print(\"Size after dropping NA:\", df.shape)\n",
    "\n",
    "    df = df.iloc[0:801309]\n",
    "\n",
    "    # Convert 'Crash Date Time' to datetime and extract features\n",
    "    df['CRASH_DATE'] = pd.to_datetime(df['CRASH_DATE'])\n",
    "    df['Hour'] = df['CRASH_DATE'].dt.hour\n",
    "    df['DayOfWeek'] = df['CRASH_DATE'].dt.dayofweek\n",
    "    df['Month'] = df['CRASH_DATE'].dt.month\n",
    "\n",
    "    # Define rush hours (for example, 6-9 AM and 4-7 PM)\n",
    "    morning_rush = (df['Hour'] >= 6) & (df['Hour'] <= 9)\n",
    "    evening_rush = (df['Hour'] >= 16) & (df['Hour'] <= 19)\n",
    "    df['Rush_Hour'] = (morning_rush | evening_rush).astype(int)\n",
    "\n",
    "    cat = ['Weather', 'Road Condition']\n",
    "    weather_filter = ['CLEAR','CLOUDY/OVERCAST','FOG/SMOKE/HAZE','OTHER','RAIN','SLEET/HAIL','SNOW','FREEZING RAIN/DRIZZLE']\n",
    "    road_cond_filter = ['DRY','ICE','OTHER','WET']\n",
    "    df = df[df['Weather'].isin(weather_filter) & df['Road Condition'].isin(road_cond_filter)]\n",
    "    \n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    # One-hot encoding is a common method for \n",
    "    # converting categorical variables into a binary matrix representation. \n",
    "    # Fit the encoder on the full dataset for the categorical columns\n",
    "    encoder.fit(df[cat])\n",
    "\n",
    "    # applies mapping and converts categorical to numerical value\n",
    "    encoded_features = encoder.transform(df[cat])\n",
    "\n",
    "    X_numerical = df[['Hour']].values\n",
    "\n",
    "    # Create feature matrix by combining encoded categorical features and numerical features\n",
    "    X = np.concatenate((encoded_features, X_numerical), axis=1)\n",
    "\n",
    "    # The target variable 'Rush_Hour' is already binary (0 or 1), so no need to encode\n",
    "    y = df['Rush_Hour'].values\n",
    "\n",
    "    # combine encoded features, x numarical, y target to df for future need \n",
    "    encdfeatnames = encoder.get_feature_names_out(cat)\n",
    "    df_encd = pd.DataFrame(encoded_features, columns = encdfeatnames)\n",
    "    df_num = pd.DataFrame(X_numerical, columns = ['Hour'])\n",
    "    df_rush_hour = pd.DataFrame(y, columns = ['Rush_Hour'])\n",
    "    \n",
    "    df_combined = pd.concat([df_encd, df_num, df_rush_hour], axis=1)\n",
    "    df_combined.to_excel(\"Laptop2_preprocessedDS.xlsx\", index=False, engine='openpyxl')  \n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print(\"Data preprocessing complete.\")\n",
    "    \n",
    "    print(\"Compiling and training the model.\")\n",
    "    # for binary classification task\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=12)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Model accuracy: {accuracy}\")    \n",
    "    \n",
    "    return model, X_test, y_test\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \n",
    "    # Evaluating on X_test checks model accuracy on new data;\n",
    "    # 0.5 is a threshold for binary outcomes\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    \n",
    "    print(\"range(len(y_test)\")\n",
    "    print(range(len(y_test)))\n",
    "    print(\"range(len(y_pred)\")\n",
    "    print(range(len(y_pred)))\n",
    "          \n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plotting the observed (actual) values vs predicted probabilities\n",
    "    observed = plt.scatter(range(len(y_test)), y_test, color='red', label='Observed', alpha=0.5, marker='o')\n",
    "\n",
    "    # Plotting the same for predicted binary outcomes for comparison\n",
    "    predicted = plt.scatter(range(len(y_pred_binary)), y_pred_binary, color='blue', label='Predicted', alpha=0.5, marker='x')\n",
    "\n",
    "    # Draw a line of best fit for observed vs predicted\n",
    "    m, b = np.polyfit(range(len(y_test)), y_pred, 1)\n",
    "    plt.plot(range(len(y_test)), m*np.arange(len(y_test)) + b, color='green', label='Fit')\n",
    "\n",
    "    # Enhance plot\n",
    "    plt.title('Observed vs Predicted')\n",
    "    plt.xlabel('Samples')\n",
    "    plt.ylabel('Output label')\n",
    "    plt.legend(handles=[observed, predicted])\n",
    "    plt.show() \n",
    "    \n",
    "    # confusion matrix\n",
    "    # https://neptune.ai/blog/evaluation-metrics-binary-classification\n",
    "    print(\"confusion matrix\")\n",
    "    cm = confusion_matrix(y_test, y_pred_binary)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    false_positive_rate = fp / (fp + tn)\n",
    "    print(f\"False Positive Rate: {false_positive_rate}\")\n",
    "    false_negative_rate = fn / (tp + fn)\n",
    "    print(f\"False Negative Rate: {false_negative_rate}\")\n",
    "    true_negative_rate = tn / (tn + fp)\n",
    "    print(f\"True Negative Rate: {true_negative_rate}\")   \n",
    "\n",
    "    if (tn + fn) > 0:\n",
    "        negative_predictive_value = tn / (tn + fn)\n",
    "        print(f\"Negative Prediction Value: {negative_predictive_value}\")\n",
    "    else:\n",
    "        print(\"Negative Prediction Value: Undefined (no true or false negatives)\")\n",
    "    \n",
    "    if (tp + fp) == 0:\n",
    "        false_discovery_rate = 0\n",
    "        print(f\"False Discovery Rate: {false_discovery_rate}\")  \n",
    "    else:\n",
    "        false_discovery_rate = fp/ (tp + fp)\n",
    "        print(f\"False Discovery Rate: {false_discovery_rate}\") \n",
    "        \n",
    "    recall = recall_score(y_test, y_pred_binary) \n",
    "    print(f\"recall: {recall}\")\n",
    "    precision = precision_score(y_test, y_pred_binary) \n",
    "    print(f\"precision: {precision}\")    \n",
    "    fbeta = fbeta_score(y_test, y_pred_binary, beta=2)\n",
    "    print(f\"fbeta: {fbeta}\")\n",
    "    f1= f1_score(y_test, y_pred_binary)\n",
    "    print(f\"f1: {f1}\")\n",
    "    f2 = fbeta_score(y_test, y_pred_binary, beta = 2)\n",
    "    print(f\"f2: {f2}\")\n",
    "    cohen_kappa = cohen_kappa_score(y_test, y_pred_binary)\n",
    "    print(f\"cohen_kappa: {cohen_kappa}\") \n",
    "    \n",
    "    \n",
    "    # Classification report (precision, recall, F1-score)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred_binary, zero_division=0))\n",
    "\n",
    "    # ROC-AUC score\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(y_test, y_pred)\n",
    "        print(f\"ROC_AUC score: {roc_auc}\")\n",
    "        \n",
    "    except:\n",
    "        # Handle cases where ROC-AUC can not be computed\n",
    "        print(\"ROC-AUC score cannot be computed for this model configuration.\")\n",
    "        \n",
    "    # Matthews Correlation Coefficient (MCC)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred_binary)\n",
    "    print(f\"Matthews Correlation Coefficient (MCC): {mcc}\")\n",
    "    \n",
    "def send_updated_weights(client_socket, model):\n",
    "    updated_weights = model.get_weights()\n",
    "    data = pickle.dumps(updated_weights)\n",
    "    # Send the size of the pickled data first\n",
    "    client_socket.sendall(len(data).to_bytes(8, byteorder='big'))\n",
    "    # Then send the pickled data\n",
    "    client_socket.sendall(data)\n",
    "    print(\"Updated weights sent.\")    \n",
    "    \n",
    "# client_program():\n",
    "print(\"Client program started.\")\n",
    "host = '127.0.0.1'   # Server address\n",
    "port = 5300  # Server port number\n",
    "\n",
    "client_socket = socket.socket()\n",
    "client_socket.connect((host, port))\n",
    "print(\"Connected to server at {}:{}\".format(host, port))\n",
    "\n",
    "# Receive the initial model from the server\n",
    "model = receive_model(client_socket)\n",
    "# Train and preprocess the model\n",
    "model, X_test, y_test = preprocess_data_Train_Model(model) \n",
    "print(\"Evaluating model after traning with initial model\")\n",
    "evaluate_model(model, X_test, y_test)\n",
    "\n",
    "# Send the updated weights back to the server\n",
    "send_updated_weights(client_socket, model)\n",
    "\n",
    "print('Receiving the global model from the server')     \n",
    "# Receive the global model from the server\n",
    "model = receive_model(client_socket)  \n",
    "print(\"*****************************************************\")\n",
    "print(\"Evaluating model after traning with global model\")\n",
    "print(\"*****************************************************\")\n",
    "# use X_test, y_test from above as server don't send the data set info\n",
    "evaluate_model(model, X_test, y_test)\n",
    "\n",
    "client_socket.close()\n",
    "print(\"Client program completed and connection closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21e7fc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e94b1b1-036b-4aec-ac63-c981c50994cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
